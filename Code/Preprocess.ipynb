{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary code for paper submission: 'Tracing Semantic Variation in Slang'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the supplementary data pre-processing code for 'Tracing Semantic Variation in Slang'. Since we cannot publically release all entries from Green's Dictionary of Slang (GDoS) due to copyright terms, this note book illustrates how we pre-process raw data obtained from https://greensdictofslang.com/ and turn the data into a format that can be used to reproduce our experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of non-standard Python packages you'll need. All of which can be obtained using *pip install*.\n",
    "\n",
    "- numpy\n",
    "- bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import GSD_Definition, GSD_Word\n",
    "from process import process_GSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we include the raw html dumps for 3 dictionary entries for the slang word *beast*. Each file is named after its hash tag organized by the original dictionary. The original entries can be found on the following webpages:\n",
    "\n",
    "https://greensdictofslang.com/entry/23sqfua\n",
    "\n",
    "https://greensdictofslang.com/entry/xzzdtua\n",
    "\n",
    "https://greensdictofslang.com/entry/3e7vqxq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not first crawl our directory for these hash tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_hash = [s[:-5] for s in glob.glob('*.html')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(word_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pre-processing function will then take in a list of hash tags and process the respective html files. A pickle file will be generated for each word entry. Note that we do not collapse homonyms (i.e. same word form with multiple word entries) until the actual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "process_GSD(word_hash, input_dir = \"\", output_dir = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should generate 3 pickle files which we now load for further pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [pickle.load(open(h+'.pickle', 'rb')) for h in word_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code filters the reference entries according to the set of regions that we are interested in (in our case, US and UK). It also tries to automatically extract valid example usage sentences from the reference entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['[US]', '[UK]']\n",
    "#regions = ['[US]', '[UK]', '[Aus]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "punctuations = '!\\'\"#$%&()\\*\\+,-\\./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "re_punc = re.compile(r\"[\"+punctuations+r\"]+\")\n",
    "re_space = re.compile(r\" +\")\n",
    "\n",
    "re_extract_quote = re.compile(r\"[1-9/]+:\")\n",
    "re_extract_quote_all = re.compile(r\"[1-9/]+:.*$\")\n",
    "\n",
    "def proc_quote_sent(sent):\n",
    "    return re_extract_quote.sub(' ', re_extract_quote_all.findall(sent)[0]).strip()\n",
    "\n",
    "def validate_quote_sent(word, sent):\n",
    "    tokens = [s.lower() for s in re_space.sub(' ', re_punc.sub('', sent)).split(' ')]\n",
    "    return word.lower() in tokens\n",
    "\n",
    "data_proc = []\n",
    "\n",
    "for i in trange(len(data)):\n",
    "    w = data[i]\n",
    "    if w.is_abbr():\n",
    "        continue\n",
    "    d_list = []\n",
    "    for d in w.definitions:\n",
    "        stamps = d.stamps\n",
    "        region_set = set([s[1] for s in stamps])\n",
    "        if np.any([r in region_set for r in regions]):\n",
    "            new_stamps = [s for s in stamps if np.any([r==s[1] in region_set for r in regions])]\n",
    "            new_def = GSD_Definition(d.def_sent)\n",
    "            new_def.stamps = new_stamps\n",
    "            new_def.contexts = {key:value for key, value in d.contexts.items() if key in new_stamps}\n",
    "            d_list.append(new_def)\n",
    "    if len(d_list) > 0:\n",
    "        new_word = GSD_Word(w.word.replace(\"\\\\xe2\\\\x80\\\\x99\", \"'\").replace(\"\\\\xe2\\\\x80\\\\x98\", \"'\"), w.pos, w.homonym)\n",
    "        new_word.definitions = d_list\n",
    "        data_proc.append(new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the data looks after after pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = [print(d) for d in data_proc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the pre-processed data to be used for experiments. See the notebook *Trace.ipynb* in the code package for how this can be used to reproduce results in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GSD_sample_data.npy', data_proc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
